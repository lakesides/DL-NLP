
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Attention的原理和实现 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="2.3.5 BeamSearch的原理和实现.html" />
    
    
    <link rel="prev" href="2.3.3 seq2seq实现闲聊机器人.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    神经网络和pytorch
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../1.1 深度学习和神经网络/">
            
                <a href="../1.1 深度学习和神经网络/">
            
                    
                    1.1 深度学习和神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="../1.1 深度学习和神经网络/1.1.1 深度学习的介绍.html">
            
                <a href="../1.1 深度学习和神经网络/1.1.1 深度学习的介绍.html">
            
                    
                    深度学习的介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="../1.1 深度学习和神经网络/1.1.2 神经网络的介绍.html">
            
                <a href="../1.1 深度学习和神经网络/1.1.2 神经网络的介绍.html">
            
                    
                    神经网络的介绍
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../1.2 pytorch/">
            
                <a href="../1.2 pytorch/">
            
                    
                    1.2 pytorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="../1.2 pytorch/1.2.1 pytorch的介绍和安装.html">
            
                <a href="../1.2 pytorch/1.2.1 pytorch的介绍和安装.html">
            
                    
                    pytorch的介绍和安装
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="../1.2 pytorch/1.2.2 pytorch的入门使用.html">
            
                <a href="../1.2 pytorch/1.2.2 pytorch的入门使用.html">
            
                    
                    pytorch的入门使用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="../1.2 pytorch/1.2.3 梯度下降和反向传播原理.html">
            
                <a href="../1.2 pytorch/1.2.3 梯度下降和反向传播原理.html">
            
                    
                    梯度下降和反向传播原理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="../1.2 pytorch/1.2.4 手动完成线性回归 .html">
            
                <a href="../1.2 pytorch/1.2.4 手动完成线性回归 .html">
            
                    
                    手动完成线性回归 
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="../1.2 pytorch/1.2.5调用 pytorch API完成线性回归.html">
            
                <a href="../1.2 pytorch/1.2.5调用 pytorch API完成线性回归.html">
            
                    
                    调用 pytorch API完成线性回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.6" data-path="../1.2 pytorch/1.2.6 pytorch中的数据加载.html">
            
                <a href="../1.2 pytorch/1.2.6 pytorch中的数据加载.html">
            
                    
                    pytorch中的数据加载
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.7" data-path="../1.2 pytorch/1.2.7 使用pytorch实现手写数字识别.html">
            
                <a href="../1.2 pytorch/1.2.7 使用pytorch实现手写数字识别.html">
            
                    
                    使用pytorch实现手写数字识别
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="../1.3 循环神经网络/">
            
                <a href="../1.3 循环神经网络/">
            
                    
                    1.3 循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="../1.3 循环神经网络/1.3.1 循环神经网络基础.html">
            
                <a href="../1.3 循环神经网络/1.3.1 循环神经网络基础.html">
            
                    
                    循环神经网络基础
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.2" data-path="../1.3 循环神经网络/1.3.2 文本情感分类.html">
            
                <a href="../1.3 循环神经网络/1.3.2 文本情感分类.html">
            
                    
                    文本情感分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.3" data-path="../1.3 循环神经网络/1.3.3 循环神经网络.html">
            
                <a href="../1.3 循环神经网络/1.3.3 循环神经网络.html">
            
                    
                    循环神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4" data-path="../1.3 循环神经网络/1.3.4 循环神经网络实现情感分类.html">
            
                <a href="../1.3 循环神经网络/1.3.4 循环神经网络实现情感分类.html">
            
                    
                    循环神经网络实现情感分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.5" data-path="../1.3 循环神经网络/1.3.5 神经网络中的序列化容器.html">
            
                <a href="../1.3 循环神经网络/1.3.5 神经网络中的序列化容器.html">
            
                    
                    神经网络中的序列化容器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.6" data-path="../1.3 循环神经网络/1.3.6 神经网络实现分词.html">
            
                <a href="../1.3 循环神经网络/1.3.6 神经网络实现分词.html">
            
                    
                    神经网络实现分词
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    项目实现
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../2.1 项目准备/">
            
                <a href="../2.1 项目准备/">
            
                    
                    2.1 项目装备
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="../2.1 项目准备/2.1.1 走进聊天机器人.html">
            
                <a href="../2.1 项目准备/2.1.1 走进聊天机器人.html">
            
                    
                    走进聊天机器人
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="../2.1 项目准备/2.1.2 需求分析和流程介绍.html">
            
                <a href="../2.1 项目准备/2.1.2 需求分析和流程介绍.html">
            
                    
                    需求分析和流程介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="../2.1 项目准备/2.1.3 环境准备.html">
            
                <a href="../2.1 项目准备/2.1.3 环境准备.html">
            
                    
                    环境准备
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="../2.1 项目准备/2.1.4 语料准备.html">
            
                <a href="../2.1 项目准备/2.1.4 语料准备.html">
            
                    
                    语料准备
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="../2.1 项目准备/2.1.5 文本分词.html">
            
                <a href="../2.1 项目准备/2.1.5 文本分词.html">
            
                    
                    文本分词
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="../2.1 项目准备/2.1.6 动手练习.html">
            
                <a href="../2.1 项目准备/2.1.6 动手练习.html">
            
                    
                    动手练习
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../2.2 fasttext文本分类/">
            
                <a href="../2.2 fasttext文本分类/">
            
                    
                    2.2 FastText文本分类
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="../2.2 fasttext文本分类/2.2.1 分类的目的和方法.html">
            
                <a href="../2.2 fasttext文本分类/2.2.1 分类的目的和方法.html">
            
                    
                    分类的目的和方法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="../2.2 fasttext文本分类/2.2.2 fasttext实现文本分类.html">
            
                <a href="../2.2 fasttext文本分类/2.2.2 fasttext实现文本分类.html">
            
                    
                    fasttext实现文本分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="../2.2 fasttext文本分类/2.2.3 fasttext的原理剖析.html">
            
                <a href="../2.2 fasttext文本分类/2.2.3 fasttext的原理剖析.html">
            
                    
                    fasttext的原理剖析
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="./">
            
                <a href="./">
            
                    
                    2.3 Seq2Seq模型和闲聊机器人
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="2.3.1 闲聊机器人的介绍.html">
            
                <a href="2.3.1 闲聊机器人的介绍.html">
            
                    
                    闲聊机器人的介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="2.3.2 seq2seq模型的原理.html">
            
                <a href="2.3.2 seq2seq模型的原理.html">
            
                    
                    seq2seq模型的原理
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="2.3.3 seq2seq实现闲聊机器人.html">
            
                <a href="2.3.3 seq2seq实现闲聊机器人.html">
            
                    
                    seq2seq实现闲聊机器人
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.3.4" data-path="2.3.4 Attention的原理和实现.html">
            
                <a href="2.3.4 Attention的原理和实现.html">
            
                    
                    Attention的原理和实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.5" data-path="2.3.5 BeamSearch的原理和实现.html">
            
                <a href="2.3.5 BeamSearch的原理和实现.html">
            
                    
                    BeamSearch的原理和实现
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.6" data-path="2.3.6 闲聊机器人的优化.html">
            
                <a href="2.3.6 闲聊机器人的优化.html">
            
                    
                    闲聊机器人的优化
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="../2.4 QA机器人/">
            
                <a href="../2.4 QA机器人/">
            
                    
                    2.4 QA机器人
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.4.1" data-path="../2.4 QA机器人/2.4.1 QA机器人介绍.html">
            
                <a href="../2.4 QA机器人/2.4.1 QA机器人介绍.html">
            
                    
                    QA机器人介绍.md
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4.2" data-path="../2.4 QA机器人/2.4.2 QA机器人的召回.html">
            
                <a href="../2.4 QA机器人/2.4.2 QA机器人的召回.html">
            
                    
                    QA机器人的召回.md
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4.3" data-path="../2.4 QA机器人/2.4.3 召回优化.html">
            
                <a href="../2.4 QA机器人/2.4.3 召回优化.html">
            
                    
                    召回优化.md
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4.4" data-path="../2.4 QA机器人/2.4.4 QA机器人排序模型.html">
            
                <a href="../2.4 QA机器人/2.4.4 QA机器人排序模型.html">
            
                    
                    QA机器人排序模型.md
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4.5" data-path="../2.4 QA机器人/2.4.5 代码的封装和提供接口.html">
            
                <a href="../2.4 QA机器人/2.4.5 代码的封装和提供接口.html">
            
                    
                    代码的封装和提供接口.md
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    补充
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../补充/HMM.html">
            
                <a href="../补充/HMM.html">
            
                    
                    HMM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../补充/MEMM和CRF.html">
            
                <a href="../补充/MEMM和CRF.html">
            
                    
                    MEMM和CRF
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../补充/最大匹配法.html">
            
                <a href="../补充/最大匹配法.html">
            
                    
                    最大匹配法
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Attention的原理和实现</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="attention&#x7684;&#x539F;&#x7406;&#x548C;&#x5B9E;&#x73B0;">Attention&#x7684;&#x539F;&#x7406;&#x548C;&#x5B9E;&#x73B0;</h1>
<h2 id="&#x76EE;&#x6807;">&#x76EE;&#x6807;</h2>
<ol>
<li>&#x77E5;&#x9053;Attention&#x7684;&#x4F5C;&#x7528;</li>
<li>&#x77E5;&#x9053;Attention&#x7684;&#x5B9E;&#x73B0;&#x673A;&#x5236;</li>
<li>&#x80FD;&#x591F;&#x4F7F;&#x7528;&#x4EE3;&#x7801;&#x5B8C;&#x6210;Attention&#x4EE3;&#x7801;&#x7684;&#x7F16;&#x5199;</li>
</ol>
<h2 id="1-attention&#x7684;&#x4ECB;&#x7ECD;">1. Attention&#x7684;&#x4ECB;&#x7ECD;</h2>
<p>&#x5728;&#x666E;&#x901A;&#x7684;RNN&#x7ED3;&#x6784;&#x4E2D;&#xFF0C;Encoder&#x9700;&#x8981;&#x628A;&#x4E00;&#x4E2A;&#x53E5;&#x5B50;&#x8F6C;&#x5316;&#x4E3A;&#x4E00;&#x4E2A;&#x5411;&#x91CF;&#xFF0C;&#x7136;&#x540E;&#x5728;Decoder&#x4E2D;&#x4F7F;&#x7528;&#xFF0C;&#x8FD9;&#x5C31;&#x8981;&#x6C42;Encoder&#x628A;&#x6E90;&#x53E5;&#x5B50;&#x4E2D;&#x6240;&#x6709;&#x7684;&#x4FE1;&#x606F;&#x90FD;&#x5305;&#x542B;&#x8FDB;&#x53BB;&#xFF0C;&#x4F46;&#x662F;&#x5F53;&#x53E5;&#x5B50;&#x957F;&#x5EA6;&#x8FC7;&#x957F;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8FD9;&#x4E2A;&#x8981;&#x6C42;&#x5C31;&#x5F88;&#x96BE;&#x8FBE;&#x5230;&#xFF0C;&#x6216;&#x8005;&#x8BF4;&#x4F1A;&#x4EA7;&#x751F;&#x74F6;&#x9888;&#xFF08;&#x6BD4;&#x5982;&#xFF0C;&#x8F93;&#x5165;&#x4E00;&#x7BC7;&#x6587;&#x7AE0;&#x7B49;&#x573A;&#x957F;&#x5185;&#x5BB9;&#xFF09;&#xFF0C;&#x5F53;&#x7136;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x66F4;&#x6DF1;&#x7684;RNN&#x548C;&#x5927;&#x591A;&#x7684;&#x5355;&#x5143;&#x6765;&#x89E3;&#x51B3;&#x8FD9;&#x4E2A;&#x95EE;&#x9898;&#xFF0C;&#x4F46;&#x662F;&#x8FD9;&#x6837;&#x7684;&#x4EE3;&#x4EF7;&#x4E5F;&#x5F88;&#x5927;&#x3002;&#x90A3;&#x4E48;&#x6709;&#x6CA1;&#x6709;&#x4EC0;&#x4E48;&#x65B9;&#x6CD5;&#x80FD;&#x591F;&#x4F18;&#x5316;&#x73B0;&#x6709;&#x7684;RNN&#x7ED3;&#x6784;&#x5462;&#xFF1F;</p>
<p>&#x4E3A;&#x6B64;&#xFF0C;Bahdanau&#x7B49;&#x4EBA;&#x5728;2015&#x5E74;&#x63D0;&#x51FA;&#x4E86;<code>Attenion</code>&#x673A;&#x5236;&#xFF0C;<code>Attention</code>&#x7FFB;&#x8BD1;&#x6210;&#x4E3A;&#x4E2D;&#x6587;&#x53EB;&#x505A;&#x6CE8;&#x610F;&#x529B;&#xFF0C;&#x628A;&#x8FD9;&#x79CD;&#x6A21;&#x578B;&#x79F0;&#x4E3A;<code>Attention based model</code>&#x3002;&#x5C31;&#x50CF;&#x6211;&#x4EEC;&#x81EA;&#x5DF1;&#x770B;&#x5230;&#x4E00;&#x526F;&#x753B;&#xFF0C;&#x6211;&#x4EEC;&#x80FD;&#x591F;&#x5F88;&#x5FEB;&#x7684;&#x8BF4;&#x51FA;&#x753B;&#x7684;&#x4E3B;&#x8981;&#x5185;&#x5BB9;&#xFF0C;&#x800C;&#x5FFD;&#x7565;&#x753B;&#x4E2D;&#x7684;&#x80CC;&#x666F;&#xFF0C;&#x56E0;&#x4E3A;&#x6211;&#x4EEC;&#x6CE8;&#x610F;&#x7684;&#xFF0C;&#x66F4;&#x5173;&#x6CE8;&#x7684;&#x5F80;&#x5F80;&#x662F;&#x5176;&#x4E2D;&#x7684;&#x4E3B;&#x8981;&#x5185;&#x5BB9;&#x3002;</p>
<p>&#x901A;&#x8FC7;&#x8FD9;&#x79CD;&#x65B9;&#x5F0F;&#xFF0C;&#x5728;&#x6211;&#x4EEC;&#x7684;RNN&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x6709;&#x901A;&#x8FC7;LSTM&#x6216;&#x8005;&#x662F;GRU&#x5F97;&#x5230;&#x7684;&#x6240;&#x6709;&#x4FE1;&#x606F;&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E9B;&#x4FE1;&#x606F;&#x4E2D;&#x53EA;&#x53BB;&#x5173;&#x6CE8;&#x91CD;&#x70B9;&#xFF0C;&#x800C;&#x4E0D;&#x9700;&#x8981;&#x5728;Decoder&#x7684;&#x6BCF;&#x4E2A;time step&#x4F7F;&#x7528;&#x5168;&#x90E8;&#x7684;encoder&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x8FD9;&#x6837;&#x5C31;&#x53EF;&#x4EE5;&#x89E3;&#x51B3;&#x7B2C;&#x4E00;&#x6BB5;&#x6240;&#x8BF4;&#x7684;&#x95EE;&#x9898;&#x4E86;</p>
<p>&#x90A3;&#x4E48;&#x73B0;&#x5728;&#x8981;&#x8BB2;&#x7684;<code>Attention&#x673A;&#x5236;</code>&#x5C31;&#x80FD;&#x591F;&#x5E2E;&#x52A9;&#x6211;&#x4EEC;&#x89E3;&#x51B3;&#x8FD9;&#x4E2A;&#x95EE;&#x9898;</p>
<h2 id="2-attenion&#x7684;&#x5B9E;&#x73B0;&#x673A;&#x5236;">2. Attenion&#x7684;&#x5B9E;&#x73B0;&#x673A;&#x5236;</h2>
<p>&#x5047;&#x8BBE;&#x6211;&#x4EEC;&#x73B0;&#x5728;&#x6709;&#x4E00;&#x4E2A;&#x6587;&#x672C;&#x7FFB;&#x8BD1;&#x7684;&#x9700;&#x6C42;&#xFF0C;&#x5373;<code>&#x673A;&#x5668;&#x5B66;&#x4E60;</code>&#x7FFB;&#x8BD1;&#x4E3A;<code>machine learning</code>&#x3002;&#x90A3;&#x4E48;&#x8FD9;&#x4E2A;&#x8FC7;&#x7A0B;&#x901A;&#x8FC7;&#x524D;&#x9762;&#x6240;&#x5B66;&#x4E60;&#x7684;Seq2Seq&#x5C31;&#x53EF;&#x4EE5;&#x5B9E;&#x73B0;</p>
<p><img src="../images/2.3/attention1.png" alt=""></p>
<p>&#x4E0A;&#x56FE;&#x7684;&#x5DE6;&#x8FB9;&#x662F;Encoder&#xFF0C;&#x80FD;&#x591F;&#x5F97;&#x5230;<code>hidden_state</code>&#x5728;&#x53F3;&#x8FB9;&#x4F7F;&#x7528;</p>
<p>Deocder&#x4E2D;<strong>&#x84DD;&#x8272;&#x65B9;&#x6846;</strong>&#x4E2D;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x662F;&#x4E3A;&#x4E86;&#x63D0;&#x9AD8;&#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;&#x901F;&#x5EA6;&#x800C;&#x4F7F;&#x7528;teacher forcing&#x624B;&#x6BB5;&#xFF0C;&#x5426;&#x5219;&#x7684;&#x8BDD;&#x4F1A;&#x628A;&#x524D;&#x4E00;&#x6B21;&#x7684;&#x8F93;&#x51FA;&#x4F5C;&#x4E3A;&#x4E0B;&#x4E00;&#x6B21;&#x7684;&#x8F93;&#x5165;&#xFF08;<strong>&#x4F46;&#x662F;&#x5728;Attention&#x6A21;&#x578B;&#x4E2D;&#x4E0D;&#x518D;&#x662F;&#x8FD9;&#x6837;&#x4E86;</strong>&#xFF09;</p>
<p>&#x90A3;&#x4E48;&#x6574;&#x4E2A;&#x8FC7;&#x7A0B;&#x4E2D;&#x5982;&#x679C;&#x4F7F;&#x7528;Attention&#x5E94;&#x8BE5;&#x600E;&#x4E48;&#x505A;&#x5462;&#xFF1F;</p>
<p>&#x5728;&#x4E4B;&#x524D;&#x6211;&#x4EEC;&#x628A;encoder&#x7684;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x8F93;&#x51FA;&#xFF0C;&#x4F5C;&#x4E3A;decoder&#x7684;&#x521D;&#x59CB;&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001;&#xFF0C;&#x73B0;&#x5728;&#x6211;&#x4EEC;&#x4E0D;&#x518D;&#x8FD9;&#x6837;&#x505A;</p>
<h3 id="21-attention&#x7684;&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;">2.1 Attention&#x7684;&#x5B9E;&#x73B0;&#x8FC7;&#x7A0B;</h3>
<ol>
<li><p>&#x521D;&#x59CB;&#x5316;&#x4E00;&#x4E2A;Decoder&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001;$z_0$</p>
</li>
<li><p>&#x8FD9;&#x4E2A;$z_o$&#x4F1A;&#x548C;encoder&#x7B2C;&#x4E00;&#x4E2A;time step&#x7684;output&#x8FDB;&#x884C;match&#x64CD;&#x4F5C;&#xFF08;&#x6216;&#x8005;&#x662F;socre&#x64CD;&#x4F5C;&#xFF09;&#xFF0C;&#x5F97;&#x5230;$\alpha_0^1$ &#xFF0C;&#x8FD9;&#x91CC;&#x7684;match&#x53EF;&#x4EE5;&#x4F7F;&#x5F88;&#x591A;&#x4E2D;&#x64CD;&#x4F5C;&#xFF0C;&#x6BD4;&#x5982;&#xFF1A;</p>
<ul>
<li>z&#x548C;h&#x7684;&#x4F59;&#x5F26;&#x503C;</li>
<li>&#x662F;&#x4E00;&#x4E2A;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#xFF0C;&#x8F93;&#x5165;&#x4E3A;z&#x548C;h</li>
<li>&#x6216;&#x8005;$\alpha = h^T W z&#x200B;$&#x7B49;</li>
<li><img src="../images/2.3/Attention2.png" alt=""></li>
</ul>
</li>
<li><p>encoder&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;output&#x90FD;&#x548C;$z_0&#x200B;$&#x8FDB;&#x884C;&#x8BA1;&#x7B97;&#x4E4B;&#x540E;&#xFF0C;&#x5F97;&#x5230;&#x7684;&#x7ED3;&#x679C;&#x8FDB;&#x884C;softmax&#xFF0C;&#x8BA9;&#x4ED6;&#x4EEC;&#x7684;&#x548C;&#x4E3A;1(&#x53EF;&#x4EE5;&#x7406;&#x89E3;&#x4E3A;&#x6743;&#x91CD;)</p>
</li>
<li><p>&#x4E4B;&#x540E;&#x628A;&#x6240;&#x6709;&#x7684;softmax&#x4E4B;&#x540E;&#x7684;&#x7ED3;&#x679C;&#x548C;&#x539F;&#x6765;encoder&#x7684;&#x8F93;&#x51FA;$h_i&#x200B;$&#x8FDB;&#x884C;&#x76F8;&#x52A0;&#x6C42;&#x548C;&#x5F97;&#x5230;$c^0&#x200B;$
$$
&#x5373;&#xFF1A; c^0 = \sum\hat{\alpha}_0^ih^i
$$</p>
<ul>
<li><img src="../images/2.3/Attention3.png" alt=""></li>
</ul>
</li>
<li><p>&#x5F97;&#x5230;$c^0&#x200B;$&#x4E4B;&#x540E;&#xFF0C;&#x628A;&#x5B83;&#x4F5C;&#x4E3A;decoder&#x7684;input&#xFF0C;&#x540C;&#x548C;&#x4F20;&#x5165;&#x521D;&#x59CB;&#x5316;&#x7684;$z^0&#x200B;$&#xFF0C;&#x5F97;&#x5230;&#x7B2C;&#x4E00;&#x4E2A;time step&#x7684;&#x8F93;&#x51FA;&#x548C;hidden_state&#xFF08;$Z^1&#x200B;$&#xFF09;</p>
<ul>
<li><img src="../images/2.3/Attention4.png" alt=""></li>
</ul>
</li>
<li><p>&#x628A;$Z_1&#x200B;$&#x518D;&#x548C;&#x6240;&#x6709;&#x7684;encoder&#x7684;output&#x8FDB;&#x884C;match&#x64CD;&#x4F5C;&#xFF0C;&#x5F97;&#x5230;&#x7684;&#x7ED3;&#x679C;&#x8FDB;&#x884C;softmax&#x4E4B;&#x540E;&#x4F5C;&#x4E3A;&#x6743;&#x91CD;&#x548C;encoder&#x7684;&#x6BCF;&#x4E2A;timestep&#x7684;&#x7ED3;&#x679C;&#x76F8;&#x4E58;&#x6C42;&#x548C;&#x5F97;&#x5230;$c^1&#x200B;$</p>
</li>
<li><p>&#x518D;&#x628A;$c^1&#x200B;$&#x4F5C;&#x4E3A;decoder&#x7684;input&#xFF0C;&#x548C;$Z^1&#x200B;$&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#x5F97;&#x5230;&#x4E0B;&#x4E00;&#x4E2A;&#x8F93;&#x51FA;&#xFF0C;&#x5982;&#x6B64;&#x5FAA;&#x73AF;,&#x53EA;&#x5230;&#x6700;&#x7EC8;decoder&#x7684;output&#x4E3A;&#x7EC8;&#x6B62;&#x7B26;</p>
<ul>
<li><img src="../images/2.3/Attention5.png" alt=""></li>
</ul>
</li>
<li><p>&#x4E0A;&#x8FF0;&#x53C2;&#x8003;&#xFF1A;<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html" target="_blank">http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html</a></p>
</li>
<li><p>&#x6574;&#x4E2A;&#x8FC7;&#x7A0B;&#x5199;&#x6210;&#x6570;&#x5B66;&#x516C;&#x5F0F;&#x5982;&#x4E0B;&#xFF1A; 
$$
\begin{align<em>}
\alpha<em>{ij} &amp;= \frac{exp(score(h_i,\overline{h}_j))}{\sum exp(score(h_n,\overline{h}_m))} &amp; [attention \quad weight]\
c_i &amp;=\sum \alpha</em>{ij}\overline{h}_s  &amp; [context\quad vector] \
\alpha_i &amp;= f(c_i,h_i) = tanh(W_c[c_i;h_i]) &amp;[attenton \quad result]
\end{align</em>}
$$</p>
<ol>
<li>&#x5148;&#x8BA1;&#x7B97;attention&#x6743;&#x91CD;</li>
<li>&#x5728;&#x8BA1;&#x7B97;&#x4E0A;&#x4E0B;&#x6587;&#x5411;&#x91CF;&#xFF0C;&#x56FE;&#x4E2D;&#x7684;$c^i&#x200B;$</li>
<li>&#x6700;&#x540E;&#x8BA1;&#x7B97;&#x7ED3;&#x679C;&#xFF0C;&#x5F80;&#x5F80;&#x4F1A;&#x628A;&#x5F53;&#x524D;&#x7684;output([batch_size,1,hidden_size])&#x548C;&#x4E0A;&#x4E0B;&#x6587;&#x5411;&#x91CF;&#x8FDB;&#x884C;&#x62FC;&#x63A5;&#x7136;&#x540E;&#x4F7F;&#x7528;</li>
</ol>
</li>
</ol>
<h3 id="22-&#x4E0D;&#x540C;attention&#x7684;&#x4ECB;&#x7ECD;">2.2 &#x4E0D;&#x540C;Attention&#x7684;&#x4ECB;&#x7ECD;</h3>
<p>&#x5728;&#x4E0A;&#x8FF0;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4F7F;&#x7528;decoder&#x7684;&#x72B6;&#x6001;&#x548C;encoder&#x7684;&#x72B6;&#x6001;&#x7684;&#x8BA1;&#x7B97;&#x540E;&#x7684;&#x7ED3;&#x679C;&#x4F5C;&#x4E3A;&#x6743;&#x91CD;&#xFF0C;&#x4E58;&#x4E0A;encoder&#x6BCF;&#x4E2A;&#x65F6;&#x95F4;&#x6B65;&#x7684;&#x8F93;&#x51FA;&#xFF0C;&#x8FD9;&#x9700;&#x8981;&#x6211;&#x4EEC;&#x53BB;&#x8BAD;&#x7EC3;&#x4E00;&#x4E2A;&#x5408;&#x9002;&#x7684;match&#x51FD;&#x6570;&#xFF0C;&#x5F97;&#x5230;&#x7684;&#x7ED3;&#x679C;&#x5C31;&#x80FD;&#x591F;&#x5728;&#x4E0D;&#x540C;&#x7684;&#x65F6;&#x95F4;&#x6B65;&#x4E0A;&#x4F7F;&#x7528;&#x4E0D;&#x540C;&#x7684;encoder&#x7684;&#x76F8;&#x5173;&#x4FE1;&#x606F;&#xFF0C;&#x4ECE;&#x800C;&#x8FBE;&#x5230;&#x53EA;&#x5173;&#x6CE8;&#x67D0;&#x4E00;&#x4E2A;&#x5C40;&#x90E8;&#x7684;&#x6548;&#x679C;&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x6CE8;&#x610F;&#x529B;&#x7684;&#x6548;&#x679C;</p>
<h3 id="221-soft-attention-&#x548C;-hard-attention">2.2.1 <code>Soft-Attention &#x548C; Hard-Attention</code></h3>
<p>&#x6700;&#x5F00;&#x59CB;<code>Bahdanau</code>&#x7B49;&#x4EBA;&#x63D0;&#x51FA;&#x7684;Attention&#x673A;&#x5236;&#x901A;&#x5E38;&#x88AB;&#x79F0;&#x4E3A;<code>soft-attention</code>,&#x6240;&#x8C13;&#x7684;<code>soft-attention</code>&#x6307;&#x7684;&#x662F;encoder&#x4E2D;&#x8F93;&#x5165;&#x7684;&#x6BCF;&#x4E2A;&#x8BCD;&#x8BED;&#x90FD;&#x4F1A;&#x8BA1;&#x7B97;&#x5F97;&#x5230;&#x4E00;&#x4E2A;&#x6CE8;&#x610F;&#x529B;&#x7684;&#x6982;&#x7387;&#x3002;</p>
<p>&#x5728;&#x8FDB;&#x884C;&#x56FE;&#x50CF;&#x6355;&#x6349;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x63D0;&#x51FA;&#x4E86;&#x4E00;&#x79CD;<code>hard-attenion</code>&#x7684;&#x65B9;&#x6CD5;&#xFF0C;&#x5E0C;&#x671B;&#x76F4;&#x63A5;&#x4ECE;input&#x4E2D;&#x627E;&#x5230;&#x4E00;&#x4E2A;&#x548C;&#x8F93;&#x51FA;&#x7684;&#x67D0;&#x4E2A;&#x8BCD;&#x5BF9;&#x5E94;&#x7684;&#x90A3;&#x4E00;&#x4E2A;&#x8BCD;&#x3002;&#x4F46;&#x662F;&#x7531;&#x4E8E;NLP&#x4E2D;&#x8BCD;&#x8BED;&#x548C;&#x8BCD;&#x8BED;&#x4E4B;&#x95F4;&#x5F80;&#x5F80;&#x5B58;&#x5728;&#x8054;&#x7CFB;&#xFF0C;&#x4E0D;&#x4F1A;&#x53EA;&#x5173;&#x6CE8;&#x67D0;&#x4E00;&#x4E2A;&#x8BCD;&#x8BED;&#xFF0C;&#x6240;&#x4EE5;&#x90FD;&#x4F1A;&#x4F7F;&#x7528;soft-attention&#xFF0C;&#x6240;&#x4EE5;&#x8FD9;&#x91CC;&#x7684;&#x5C31;&#x4E0D;&#x591A;&#x4ECB;&#x7ECD;hard-attention</p>
<p><img src="../images/2.3/soft-hard attention.jpg" alt=""></p>
<h3 id="223-global-attention-&#x548C;local-attention">2.2.3 <code>Global-Attention &#x548C;Local Attention</code></h3>
<p><code>Bahdanau</code>&#x7B49;&#x4EBA;&#x63D0;&#x51FA;&#x7684;<code>Bahdanau Attention</code> &#x88AB;&#x79F0;&#x4E3A;<code>local attention</code>,&#x540E;&#x6765;<code>Luong</code>&#x7B49;&#x4EBA;&#x63D0;&#x51FA;&#x7684;<code>Luong Attention</code>&#x662F;&#x4E00;&#x79CD;&#x5168;&#x5C40;&#x7684;attenion&#x3002;</p>
<p>&#x6240;&#x8C13;&#x5168;&#x5C40;&#x7684;attenion&#x6307;&#x7684;&#x662F;&#xFF1A;&#x4F7F;&#x7528;&#x7684;&#x5168;&#x90E8;&#x7684;encoder&#x7AEF;&#x7684;&#x8F93;&#x5165;&#x7684;attenion&#x7684;&#x6743;&#x91CD;</p>
<p><code>local-attenion</code>&#x5C31;&#x662F;&#x4F7F;&#x7528;&#x4E86;&#x90E8;&#x5206;&#x7684;encoder&#x7AEF;&#x7684;&#x8F93;&#x5165;&#x7684;&#x6743;&#x91CD;(&#x5F53;&#x524D;&#x65F6;&#x95F4;&#x6B65;&#x4E0A;&#x7684;encoder&#x7684;hidden state)&#xFF0C;&#x8FD9;&#x6837;&#x53EF;&#x4EE5;&#x51CF;&#x5C11;&#x8BA1;&#x7B97;&#x91CF;&#xFF0C;&#x7279;&#x522B;&#x662F;&#x5F53;&#x53E5;&#x5B50;&#x7684;&#x957F;&#x5EA6;&#x6BD4;&#x8F83;&#x957F;&#x7684;&#x65F6;&#x5019;&#x3002;</p>
<h3 id="224--bahdanau-attention&#x548C;-luong-attenion&#x7684;&#x533A;&#x522B;">2.2.4  <code>Bahdanau Attention&#x548C; Luong Attenion</code>&#x7684;&#x533A;&#x522B;</h3>
<p>&#x533A;&#x522B;&#x5728;&#x4E8E;&#x4E24;&#x4E2A;&#x5730;&#x65B9;&#xFF1A;</p>
<ol>
<li><p>attention&#x7684;&#x8BA1;&#x7B97;&#x6570;&#x636E;&#x548C;&#x4F4D;&#x7F6E;</p>
<ol>
<li><code>Bahdanau Attention</code>&#x4F1A;&#x4F7F;&#x7528;<code>&#x524D;&#x4E00;&#x6B21;&#x7684;&#x9690;&#x85CF;</code>&#x72B6;&#x6001;&#x6765;&#x8BA1;&#x7B97;attention weight&#xFF0C;&#x6240;&#x4EE5;&#x6211;&#x4EEC;&#x4F1A;&#x5728;&#x4EE3;&#x7801;&#x4E2D;&#x7684;GRU&#x4E4B;&#x524D;&#x4F7F;&#x7528;attention&#x7684;&#x64CD;&#x4F5C;&#xFF0C;&#x540C;&#x65F6;&#x4F1A;&#x628A;attention&#x7684;&#x7ED3;&#x679C;&#x548C;word embedding&#x7684;&#x7ED3;&#x679C;&#x8FDB;&#x884C;concat&#xFF0C;&#x4F5C;&#x4E3A;GRU&#x7684;&#x8F93;&#x51FA;(&#x53C2;&#x8003;&#x7684;&#x662F;pytorch Toritul)&#x3002;Bahdanau&#x4F7F;&#x7528;&#x7684;&#x662F;&#x53CC;&#x5411;&#x7684;GRU&#xFF0C;&#x4F1A;&#x4F7F;&#x7528;&#x6B63;&#x53CD;&#x7684;encoder&#x7684;output&#x7684;concat&#x7684;&#x7ED3;&#x679C;&#x4F5C;&#x4E3A;encoder output,&#x5982;&#x4E0B;&#x56FE;&#x6240;&#x793A;<ul>
<li><img src="../images/2.3/Bahdanau.png" alt=""></li>
</ul>
</li>
<li><code>Luong Attenion</code>&#x4F7F;&#x7528;&#x7684;&#x662F;<code>&#x5F53;&#x524D;&#x4E00;&#x6B21;&#x7684;decoder&#x7684;output</code>&#x6765;&#x8BA1;&#x7B97;&#x5F97;&#x5230;attention weight&#xFF0C;&#x6240;&#x4EE5;&#x5728;&#x4EE3;&#x7801;&#x4E2D;&#x4F1A;&#x5728;GRU&#x7684;&#x540E;&#x9762;&#x8FDB;&#x884C;attention&#x7684;&#x64CD;&#x4F5C;&#xFF0C;&#x540C;&#x65F6;&#x4F1A;&#x628A;<code>context vector</code>&#x548C;gru&#x7684;&#x7ED3;&#x679C;&#x8FDB;&#x884C;concat&#x7684;&#x64CD;&#x4F5C;&#xFF0C;&#x6700;&#x7EC8;&#x7684;output&#x3002;Luong&#x4F7F;&#x7528;&#x7684;&#x662F;&#x591A;&#x5C42;GRU&#xFF0C;&#x53EA;&#x4F1A;&#x4F7F;&#x7528;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8F93;&#x51FA;(encoder output)<ul>
<li><img src="../images/2.3/Luong.png" alt=""></li>
</ul>
</li>
</ol>
</li>
<li><p>&#x8BA1;&#x7B97;attention weights&#x7684;&#x65B9;&#x6CD5;&#x4E0D;&#x540C;</p>
<ol>
<li><p><code>Bahdanau Attention</code>&#x7684;match&#x51FD;&#x6570;&#xFF0C;$a<em>i^j = v^T_a tanh (W_aZ</em>{i-1},+U_ah_j)&#x200B;$&#xFF0C;&#x8BA1;&#x7B97;&#x51FA;&#x6240;&#x6709;&#x7684;$a_i^j&#x200B;$&#x4E4B;&#x540E;&#xFF0C;&#x5728;&#x8BA1;&#x7B97;softmax&#xFF0C;&#x5F97;&#x5230;$\hat{a}_i^j&#x200B;$&#xFF0C;&#x5373;$\hat{a}_i^j = \frac{exp(a_i^j)}{\sum exp(a_i^j)}&#x200B;$</p>
<p>&#x5176;&#x4E2D;</p>
<ol>
<li>$v<em>a^T&#x662F;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x77E9;&#x9635;&#xFF0C;&#x9700;&#x8981;&#x88AB;&#x8BAD;&#x7EC3;&#xFF0C;W_a&#x662F;&#x5B9E;&#x73B0;&#x5BF9;Z</em>{i-1}&#x7684;&#x5F62;&#x72B6;&#x53D8;&#x5316;&#x200B;$&#xFF0C;</li>
<li>$U_a&#x5B9E;&#x73B0;&#x5BF9;h_j&#x7684;&#x5F62;&#x72B6;&#x53D8;&#x5316;&#xFF08;&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#xFF0C;&#x7406;&#x89E3;&#x4E3A;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#xFF0C;&#x5B9E;&#x73B0;&#x6570;&#x636E;&#x5F62;&#x72B6;&#x7684;&#x5BF9;&#x9F50;&#xFF09;&#x200B;$&#xFF0C;</li>
<li>$Z_{i-1}&#x662F;decoder&#x7AEF;&#x524D;&#x4E00;&#x6B21;&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001;&#xFF0C;h_j&#x662F;encoder&#x7684;output&#x200B;$</li>
</ol>
</li>
<li><p><code>Luong Attenion</code>&#x6574;&#x4F53;&#x6BD4;<code>Bahdanau Attention</code>&#x66F4;&#x52A0;&#x7B80;&#x5355;&#xFF0C;&#x4ED6;&#x4F7F;&#x7528;&#x4E86;&#x4E09;&#x79CD;&#x65B9;&#x6CD5;&#x6765;&#x8BA1;&#x7B97;&#x5F97;&#x5230;&#x6743;&#x91CD;</p>
<ol>
<li>&#x77E9;&#x9635;&#x4E58;&#x6CD5;&#xFF1A;general<ul>
<li>&#x76F4;&#x63A5;&#x5BF9;decoder&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x8FDB;&#x884C;&#x4E00;&#x4E2A;&#x77E9;&#x9635;&#x53D8;&#x6362;&#xFF08;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#xFF09;&#xFF0C;&#x7136;&#x540E;&#x548C;encoder outputs&#x8FDB;&#x884C;&#x77E9;&#x9635;&#x4E58;&#x6CD5;</li>
</ul>
</li>
<li>dot<ul>
<li>&#x76F4;&#x63A5;&#x5BF9;decoder&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x548C;encoder outputs&#x8FDB;&#x884C;&#x77E9;&#x9635;&#x4E58;&#x6CD5;</li>
</ul>
</li>
<li>concat<ul>
<li>&#x628A;decoder&#x7684;&#x9690;&#x85CF;&#x72B6;&#x6001;&#x548C;encoder&#x7684;output&#x8FDB;&#x884C;concat&#xFF0C;&#x628A;&#x8FD9;&#x4E2A;&#x7ED3;&#x679C;&#x4F7F;&#x7528;tanh&#x8FDB;&#x884C;&#x5904;&#x7406;&#x540E;&#x7684;&#x7ED3;&#x679C;&#x8FDB;&#x884C;&#x5BF9;&#x9F50;&#x8BA1;&#x7B97;&#x4E4B;&#x540E;&#xFF0C;&#x548C;encoder outputs&#x8FDB;&#x884C;&#x77E9;&#x9635;&#x4E58;&#x6CD5;</li>
</ul>
</li>
<li><img src="../images/2.3/scores.png" alt=""><ul>
<li>$h_t\text{&#x662F;&#x5F53;&#x524D;&#x7684;decoder hidden state,}h_s\text{&#x662F;&#x6240;&#x6709;&#x7684;encoder &#x7684;hidden state(encoder outputs)}$</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>&#x6700;&#x7EC8;&#x4E24;&#x4E2A;attention&#x7684;&#x7ED3;&#x679C;&#x533A;&#x522B;&#x5E76;&#x4E0D;&#x592A;&#x5927;&#xFF0C;&#x6240;&#x4EE5;&#x4EE5;&#x540E;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x8003;&#x8651;&#x4F7F;&#x7528;Luong attention&#x5B8C;&#x6210;&#x4EE3;&#x7801;</p>
<h2 id="3-attention&#x7684;&#x4EE3;&#x7801;&#x5B9E;&#x73B0;">3. Attention&#x7684;&#x4EE3;&#x7801;&#x5B9E;&#x73B0;</h2>
<p>&#x5B8C;&#x6210;&#x4EE3;&#x7801;&#x4E4B;&#x524D;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x786E;&#x5B9A;&#x6211;&#x4EEC;&#x7684;&#x601D;&#x8DEF;&#xFF0C;&#x901A;&#x8FC7;attention&#x7684;&#x4EE3;&#x7801;&#xFF0C;&#x9700;&#x8981;&#x5B9E;&#x73B0;&#x8BA1;&#x7B97;&#x7684;&#x662F;attention weight</p>
<p>&#x901A;&#x8FC7;&#x524D;&#x9762;&#x7684;&#x5B66;&#x4E60;&#xFF0C;&#x6211;&#x4EEC;&#x77E5;&#x9053;<code>attention_weight = f(hidden,encoder_outputs)</code>&#xFF0C;&#x4E3B;&#x8981;&#x5C31;&#x662F;&#x5B9E;&#x73B0;Luong attention&#x4E2D;&#x7684;&#x4E09;&#x79CD;&#x64CD;&#x4F5C;</p>
<p><img src="../images/2.3/attention6.png" alt=""></p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Attention</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,method,batch_size,hidden_size)</span>:</span>
        super(Attention,self).__init__()
        self.method = method
        self.hidden_size = hidden_size

        <span class="hljs-keyword">assert</span> self.method <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;dot&quot;</span>,<span class="hljs-string">&quot;general&quot;</span>,<span class="hljs-string">&quot;concat&quot;</span>],<span class="hljs-string">&quot;method &#x53EA;&#x80FD;&#x662F; dot,general,concat,&#x5F53;&#x524D;&#x662F;{}&quot;</span>.format(self.method)

        <span class="hljs-keyword">if</span> self.method == <span class="hljs-string">&quot;dot&quot;</span>:
            <span class="hljs-keyword">pass</span>
        <span class="hljs-keyword">elif</span> self.method == <span class="hljs-string">&quot;general&quot;</span>:
            self.Wa = nn.Linear(hidden_size,hidden_size,bias=<span class="hljs-keyword">False</span>)
        <span class="hljs-keyword">elif</span> self.method == <span class="hljs-string">&quot;concat&quot;</span>:
            self.Wa = nn.Linear(hidden_size*<span class="hljs-number">2</span>,hidden_size,bias=<span class="hljs-keyword">False</span>)
            self.Va = nn.Parameter(torch.FloatTensor(batch_size,hidden_size))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, hidden,encoder_outputs)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        :param hidden:[1,batch_size,hidden_size]
        :param encoder_outputs: [batch_size,seq_len,hidden_size]
        :return:
        &quot;&quot;&quot;</span>
        batch_size,seq_len,hidden_size = encoder_outputs.size()

        hidden = hidden.squeeze(<span class="hljs-number">0</span>) <span class="hljs-comment">#[batch_size,hidden_size]</span>

        <span class="hljs-keyword">if</span> self.method == <span class="hljs-string">&quot;dot&quot;</span>:
            <span class="hljs-keyword">return</span> self.dot_score(hidden,encoder_outputs)
        <span class="hljs-keyword">elif</span> self.method == <span class="hljs-string">&quot;general&quot;</span>:
            <span class="hljs-keyword">return</span> self.general_score(hidden,encoder_outputs)
        <span class="hljs-keyword">elif</span> self.method == <span class="hljs-string">&quot;concat&quot;</span>:
            <span class="hljs-keyword">return</span> self.concat_score(hidden,encoder_outputs)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_score</span><span class="hljs-params">(self,batch_size,seq_len,hidden,encoder_outputs)</span>:</span>
        <span class="hljs-comment"># &#x901F;&#x5EA6;&#x592A;&#x6162;</span>
        <span class="hljs-comment"># [batch_size,seql_len]</span>
        attn_energies = torch.zeros(batch_size,seq_len).to(config.device)
        <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> range(batch_size):
            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(seq_len):
                <span class="hljs-comment">#encoder_output : [batch_size,seq_len,hidden_size]</span>
                <span class="hljs-comment">#deocder_hidden :[batch_size,hidden_size]</span>
                <span class="hljs-comment">#torch.Size([256, 128]) torch.Size([128]) torch.Size([256, 24, 128]) torch.Size([128])</span>
                <span class="hljs-comment"># print(&quot;attn size:&quot;,hidden.size(),hidden[b,:].size(),encoder_output.size(),encoder_output[b,i].size())</span>
                    attn_energies[b,i] = hidden[b,:].dot(encoder_outputs[b,i]) <span class="hljs-comment">#dot score</span>
        <span class="hljs-keyword">return</span> F.softmax(attn_energies).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size,1,seq_len]</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dot_score</span><span class="hljs-params">(self,hidden,encoder_outputs)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        dot attention
        :param hidden:[batch_size,hidden_size] ---&gt;[batch_size,hidden_size,1]
        :param encoder_outputs: [batch_size,seq_len,hidden_size]
        :return:
        &quot;&quot;&quot;</span>
        <span class="hljs-comment">#hiiden :[hidden_size] --&gt;[hidden_size,1] &#xFF0C;encoder_output:[seq_len,hidden_size]</span>


        hidden = hidden.unsqueeze(<span class="hljs-number">-1</span>)
        attn_energies = torch.bmm(encoder_outputs, hidden)
        attn_energies = attn_energies.squeeze(<span class="hljs-number">-1</span>) <span class="hljs-comment">#[batch_size,seq_len,1] ==&gt;[batch_size,seq_len]</span>

        <span class="hljs-keyword">return</span> F.softmax(attn_energies).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size,1,seq_len]</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">general_score</span><span class="hljs-params">(self,hidden,encoder_outputs)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        general attenion
        :param batch_size:int
        :param hidden: [batch_size,hidden_size]
        :param encoder_outputs: [batch_size,seq_len,hidden_size]
        :return:
        &quot;&quot;&quot;</span>
        x = self.Wa(hidden) <span class="hljs-comment">#[batch_size,hidden_size]</span>
        x = x.unsqueeze(<span class="hljs-number">-1</span>) <span class="hljs-comment">#[batch_size,hidden_size,1]</span>
        attn_energies = torch.bmm(encoder_outputs,x).squeeze(<span class="hljs-number">-1</span>) <span class="hljs-comment">#[batch_size,seq_len,1]</span>
        <span class="hljs-keyword">return</span> F.softmax(attn_energies,dim=<span class="hljs-number">-1</span>).unsqueeze(<span class="hljs-number">1</span>)      <span class="hljs-comment"># [batch_size,1,seq_len]</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">concat_score</span><span class="hljs-params">(self,hidden,encoder_outputs)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        concat attention
        :param batch_size:int
        :param hidden: [batch_size,hidden_size]
        :param encoder_outputs: [batch_size,seq_len,hidden_size]
        :return:
        &quot;&quot;&quot;</span>
        <span class="hljs-comment">#&#x9700;&#x8981;&#x5148;&#x8FDB;&#x884C;repeat&#x64CD;&#x4F5C;&#xFF0C;&#x53D8;&#x6210;&#x548C;encoder_outputs&#x76F8;&#x540C;&#x7684;&#x5F62;&#x72B6;,&#x8BA9;&#x6BCF;&#x4E2A;batch&#x6709;seq_len&#x4E2A;hidden_size</span>
        x = hidden.repeat(<span class="hljs-number">1</span>,encoder_outputs.size(<span class="hljs-number">1</span>),<span class="hljs-number">1</span>) <span class="hljs-comment">##[batch_size,seq_len,hidden_size]</span>
        x = torch.tanh(self.Wa(torch.cat([x,encoder_outputs],dim=<span class="hljs-number">-1</span>))) <span class="hljs-comment">#[batch_size,seq_len,hidden_size*2] --&gt; [batch_size,seq_len,hidden_size]</span>
        <span class="hljs-comment">#va [batch_size,hidden_size] ---&gt; [batch_size,hidden_size,1]</span>
        attn_energis = torch.bmm(x,self.Va.unsqueeze(<span class="hljs-number">2</span>))  <span class="hljs-comment">#[batch_size,seq_len,1]</span>
        attn_energis = attn_energis.squeeze(<span class="hljs-number">-1</span>)
        <span class="hljs-comment"># print(&quot;concat attention:&quot;,attn_energis.size(),encoder_outputs.size())</span>
        <span class="hljs-keyword">return</span> F.softmax(attn_energis,dim=<span class="hljs-number">-1</span>).unsqueeze(<span class="hljs-number">1</span>) <span class="hljs-comment">#[batch_size,1,seq_len]</span>
</code></pre>
<p>&#x5B8C;&#x6210;&#x4E86;<code>attention weight</code>&#x7684;&#x8BA1;&#x7B97;&#x4E4B;&#x540E;&#xFF0C;&#x9700;&#x8981;&#x518D;&#x5BF9;&#x4EE3;&#x7801;&#x4E2D;<code>forward_step</code>&#x7684;&#x5185;&#x5BB9;&#x8FDB;&#x884C;&#x4FEE;&#x6539;</p>
<pre><code class="lang-python"> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward_step</span><span class="hljs-params">(self,decoder_input,decoder_hidden,encoder_outputs)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        :param decoder_input:[batch_size,1]
        :param decoder_hidden: [1,batch_size,hidden_size]
        :param encoder_outputs: encoder&#x4E2D;&#x6240;&#x6709;&#x7684;&#x8F93;&#x51FA;&#xFF0C;[batch_size,seq_len,hidden_size]
        :return: out:[batch_size,vocab_size],decoder_hidden:[1,batch_size,didden_size]
        &quot;&quot;&quot;</span>
        embeded = self.embedding(decoder_input)  <span class="hljs-comment">#embeded: [batch_size,1 , embedding_dim]</span>

        <span class="hljs-comment">#TODO &#x53EF;&#x4EE5;&#x628A;embeded&#x7684;&#x7ED3;&#x679C;&#x548C;&#x524D;&#x4E00;&#x6B21;&#x7684;context&#xFF08;&#x521D;&#x59CB;&#x503C;&#x4E3A;&#x5168;0tensor&#xFF09; concate&#x4E4B;&#x540E;&#x4F5C;&#x4E3A;&#x7ED3;&#x679C;</span>
        <span class="hljs-comment">#rnn_input = torch.cat((embeded, last_context.unsqueeze(0)), 2)</span>

        <span class="hljs-comment"># gru_out:[256,1, 128]  decoder_hidden: [1, batch_size, hidden_size]</span>
        gru_out,decoder_hidden = self.gru(embeded,decoder_hidden)
        gru_out = gru_out.squeeze(<span class="hljs-number">1</span>)

        <span class="hljs-comment">#TODO &#x6CE8;&#x610F;&#xFF1A;&#x5982;&#x679C;&#x662F;&#x5355;&#x5C42;&#xFF0C;&#x8FD9;&#x91CC;&#x4F7F;&#x7528;decoder_hidden&#x6CA1;&#x95EE;&#x9898;&#xFF08;output&#x548C;hidden&#x76F8;&#x540C;&#xFF09;</span>
        <span class="hljs-comment"># &#x5982;&#x679C;&#x662F;&#x591A;&#x5C42;&#xFF0C;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;GRU&#x7684;output&#x4F5C;&#x4E3A;attention&#x7684;&#x8F93;&#x5165;</span>
        <span class="hljs-comment">#&#x5F00;&#x59CB;&#x4F7F;&#x7528;attention</span>
        attn_weights = self.attn(decoder_hidden,encoder_outputs)
        <span class="hljs-comment"># attn_weights [batch_size,1,seq_len] * [batch_size,seq_len,hidden_size]</span>
        context = attn_weights.bmm(encoder_outputs) <span class="hljs-comment">#[batch_size,1,hidden_size]</span>

        gru_out = gru_out.squeeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># [batch_size,hidden_size]</span>
        context = context.squeeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size,hidden_size]</span>
        <span class="hljs-comment">#&#x628A;output&#x548C;attention&#x7684;&#x7ED3;&#x679C;&#x5408;&#x5E76;&#x5230;&#x4E00;&#x8D77;</span>
        concat_input = torch.cat((gru_out, context), <span class="hljs-number">1</span>) <span class="hljs-comment">#[batch_size,hidden_size*2]</span>

        concat_output = torch.tanh(self.concat(concat_input)) <span class="hljs-comment">#[batch_size,hidden_size]</span>

        output = F.log_softmax(self.fc(concat_output),dim=<span class="hljs-number">-1</span>) <span class="hljs-comment">#[batch_Size, vocab_size]</span>
        <span class="hljs-comment"># out = out.squeeze(1)</span>
        <span class="hljs-keyword">return</span> output,decoder_hidden,attn_weights
</code></pre>
<p>attetnion&#x7684;Bahdanau&#x5B9E;&#x73B0;&#x53EF;&#x4EE5;&#x53C2;&#x8003;&#xFF1A;<a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb" target="_blank">https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb</a></p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="2.3.3 seq2seq实现闲聊机器人.html" class="navigation navigation-prev " aria-label="Previous page: seq2seq实现闲聊机器人">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="2.3.5 BeamSearch的原理和实现.html" class="navigation navigation-next " aria-label="Next page: BeamSearch的原理和实现">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Attention的原理和实现","level":"1.3.3.4","depth":3,"next":{"title":"BeamSearch的原理和实现","level":"1.3.3.5","depth":3,"path":"2.3 Seq2Seq模型和闲聊机器人/2.3.5 BeamSearch的原理和实现.md","ref":"./2.3 Seq2Seq模型和闲聊机器人/2.3.5 BeamSearch的原理和实现.md","articles":[]},"previous":{"title":"seq2seq实现闲聊机器人","level":"1.3.3.3","depth":3,"path":"2.3 Seq2Seq模型和闲聊机器人/2.3.3 seq2seq实现闲聊机器人.md","ref":"./2.3 Seq2Seq模型和闲聊机器人/2.3.3 seq2seq实现闲聊机器人.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"2.3 Seq2Seq模型和闲聊机器人/2.3.4 Attention的原理和实现.md","mtime":"2019-05-09T01:51:43.968Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-04-15T10:44:50.879Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

